<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Doe"><title>Spark · 啟</title><meta name="description" content="Scala安装Scala
https://www.scala-lang.org/
https://www.scala-lang.org/download/2.11.8.html
设置SCALA_HOME系统变量然后把scala的bin目录添加到环境变量里面
12SCALA_HOME	C:\Progr"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">啟</a></h3><div class="description"><p>Caring is not a luxury I can afford.</p></div></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Spark</a></h3></div><div class="post-content"><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>安装Scala</p>
<p><a href="https://www.scala-lang.org/" target="_blank" rel="noopener">https://www.scala-lang.org/</a></p>
<p><a href="https://www.scala-lang.org/download/2.11.8.html" target="_blank" rel="noopener">https://www.scala-lang.org/download/2.11.8.html</a></p>
<p>设置SCALA_HOME系统变量然后把scala的bin目录添加到环境变量里面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME	C:\Program Files\Scala</span><br><span class="line">PATH		%SCALA_HOME%\bin</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scala</span></span><br><span class="line">Welcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_172).</span><br><span class="line">Type in expressions for evaluation. Or try :help.</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure>
<p>scala的lazy:没有调用的时候不使用</p>
<h4 id="安装Idea-Scala插件"><a href="#安装Idea-Scala插件" class="headerlink" title="安装Idea Scala插件"></a>安装Idea Scala插件</h4><p><a href="https://plugins.jetbrains.com/" target="_blank" rel="noopener">https://plugins.jetbrains.com/</a></p>
<p>选择对应版本的idea就可以了（插件对应的version是idea的version）</p>
<h4 id="Scala的函数"><a href="#Scala的函数" class="headerlink" title="Scala的函数"></a>Scala的函数</h4><h4 id="Scala的OOP"><a href="#Scala的OOP" class="headerlink" title="Scala的OOP"></a>Scala的OOP</h4><h2 id="Spark安装"><a href="#Spark安装" class="headerlink" title="Spark安装"></a>Spark安装</h2><h4 id="安装JAVA"><a href="#安装JAVA" class="headerlink" title="安装JAVA"></a>安装JAVA</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME	C:\Program Files\Java\jdk1.8.0_172</span><br><span class="line">PATH		%JAVA_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><p><a href="https://archive.apache.org/dist/hadoop/common/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME	D:\Software\hadoop-2.6.0</span><br><span class="line">PATH		%HADOOP_HOME%\bin</span><br></pre></td></tr></table></figure>
<p>ps: 对于windows平台需要到 <a href="https://github.com/steveloughran/winutils，下载对应版本的winutils" target="_blank" rel="noopener">https://github.com/steveloughran/winutils，下载对应版本的winutils</a></p>
<h4 id="安装Scala"><a href="#安装Scala" class="headerlink" title="安装Scala"></a>安装Scala</h4><p><a href="https://www.scala-lang.org/download/2.11.8.html" target="_blank" rel="noopener">https://www.scala-lang.org/download/2.11.8.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME	C:\Program Files\Scala</span><br><span class="line">PATH		%SCALA_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h4><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME	D:\Software\spark-2.1.0-bin-hadoop2.6</span><br><span class="line">PATH		%SPARK_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="运行Spark"><a href="#运行Spark" class="headerlink" title="运行Spark"></a>运行Spark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:4040/jobs/" target="_blank" rel="noopener">http://localhost:4040/jobs/</a></p>
<p>这样Spark就成功运行了。</p>
<p>PS For Linux:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/profile</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH</span><br><span class="line"></span><br><span class="line">export SCALA_HOME=/home/vaderwang/software/scala-2.11.8</span><br><span class="line">export PATH=$SCALA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/home/vaderwang/software/hadoop-2.7.3</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/home/vaderwang/software/spark-2.1.0</span><br><span class="line">export PATH=$SPARK_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>Spark Stand Alone运行</p>
<p>修改spark的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>
<p>在spark-env.sh中添加如下内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=localhost</span><br><span class="line">SPARK_WORKER_CORES=4</span><br><span class="line">SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_INSTANCES=1</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p> <a href="http://localhost:8080/" target="_blank" rel="noopener">http://localhost:8080/</a></p>
<p>配置单机多节点运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=localhost</span><br><span class="line">SPARK_WORKER_CORES=4</span><br><span class="line">SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_INSTANCES=2</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark://localhost:7077</span><br></pre></td></tr></table></figure>
<h3 id="Spark-Core-RDD"><a href="#Spark-Core-RDD" class="headerlink" title="Spark Core RDD"></a>Spark Core RDD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_map</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        result = rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>).collect()</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_filter</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        result = rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>).filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>).collect()</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_flat_map</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        <span class="comment"># [['hello', 'spark'], ['hello', 'java'], ['hello', 'python'], ['hello', 'kafka'], ['hello', 'storm']]</span></span><br><span class="line">        print(rdd.map(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br><span class="line">        <span class="comment"># ['hello', 'spark', 'hello', 'java', 'hello', 'python', 'hello', 'kafka', 'hello', 'storm']</span></span><br><span class="line">        print(rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br><span class="line">        split__map = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        print(split__map.collect())</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_group_by_key</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># [&#123;'java': [1]&#125;, &#123;'python': [1]&#125;, &#123;'storm': [1]&#125;, &#123;'hello': [1, 1, 1, 1, 1]&#125;, &#123;'spark': [1]&#125;, &#123;'kafka': [1]&#125;]</span></span><br><span class="line">        print(map_rdd.groupByKey().map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: list(x[<span class="number">1</span>])&#125;).collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_reduce_by_key</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># [&#123;'java': [1]&#125;, &#123;'python': [1]&#125;, &#123;'storm': [1]&#125;, &#123;'hello': [1, 1, 1, 1, 1]&#125;, &#123;'spark': [1]&#125;, &#123;'kafka': [1]&#125;]</span></span><br><span class="line">        group_rdd = map_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">        result = group_rdd.map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="keyword">False</span>).map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>]))</span><br><span class="line">        print(result.collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">group_by_key_word_count</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        result = map_rdd.groupByKey().map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: sum(list(x[<span class="number">1</span>]))&#125;)</span><br><span class="line">        print(result.collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_union</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    my_reduce_by_key()</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir)\</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    output = counts.collect()</span><br><span class="line">    print(output)</span><br><span class="line">    sc.stop()</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_file</span><span class="params">(file_dir, save_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir) \</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>)) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b).saveAsTextFile(save_dir)</span><br><span class="line">    <span class="comment"># output = counts.collect()</span></span><br><span class="line">    <span class="comment"># print(counts)</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topN</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir) \</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>)) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b)\</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="keyword">False</span>).map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).take(<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># output = counts.collect()</span></span><br><span class="line">    print(counts)</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    data = sc.textFile(file_dir)</span><br><span class="line">    data = data.map(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)[<span class="number">1</span>])</span><br><span class="line">    total = data.map(<span class="keyword">lambda</span> age: int(age)).reduce(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    avg = total / data.count()</span><br><span class="line">    print(avg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># main()</span></span><br><span class="line">    <span class="comment"># word_count('file:///home/vaderwang/PycharmProjects/Spark/*.txt')</span></span><br><span class="line">    <span class="comment"># save_file('file:///home/vaderwang/PycharmProjects/Spark/*.txt', 'file:///home/vaderwang/PycharmProjects/Spark/wc')</span></span><br><span class="line">    <span class="comment"># topN('file:///home/vaderwang/PycharmProjects/Spark/hello1.txt')</span></span><br><span class="line">    average(<span class="string">'file:///home/vaderwang/PycharmProjects/Spark/hello1.txt'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>接下来主要讲解sparkSQL和DataFrame</p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>Spark MLlib</p>
<p>Spark GraphX</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2019-01-16</span><i class="fa fa-tag"></i></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2019/01/16/Spark/,啟,Spark,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2019/01/20/HIVE/" title="HIVE">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2019/01/07/HBase/" title="HBase">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>