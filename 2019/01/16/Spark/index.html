<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Doe"><title>Spark · 啟</title><meta name="description" content="Scala安装Scala
https://www.scala-lang.org/
https://www.scala-lang.org/download/2.11.8.html
设置SCALA_HOME系统变量然后把scala的bin目录添加到环境变量里面
12SCALA_HOME	C:\Progr"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">啟</a></h3><div class="description"><p>Caring is not a luxury I can afford.</p></div></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Spark</a></h3></div><div class="post-content"><h2 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h2><p>安装Scala</p>
<p><a href="https://www.scala-lang.org/" target="_blank" rel="noopener">https://www.scala-lang.org/</a></p>
<p><a href="https://www.scala-lang.org/download/2.11.8.html" target="_blank" rel="noopener">https://www.scala-lang.org/download/2.11.8.html</a></p>
<p>设置SCALA_HOME系统变量然后把scala的bin目录添加到环境变量里面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME	C:\Program Files\Scala</span><br><span class="line">PATH		%SCALA_HOME%\bin</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scala</span></span><br><span class="line">Welcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_172).</span><br><span class="line">Type in expressions for evaluation. Or try :help.</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure>
<p>scala的lazy:没有调用的时候不使用</p>
<h4 id="安装Idea-Scala插件"><a href="#安装Idea-Scala插件" class="headerlink" title="安装Idea Scala插件"></a>安装Idea Scala插件</h4><p><a href="https://plugins.jetbrains.com/" target="_blank" rel="noopener">https://plugins.jetbrains.com/</a></p>
<p>选择对应版本的idea就可以了（插件对应的version是idea的version）</p>
<h4 id="Scala的函数"><a href="#Scala的函数" class="headerlink" title="Scala的函数"></a>Scala的函数</h4><h4 id="Scala的OOP"><a href="#Scala的OOP" class="headerlink" title="Scala的OOP"></a>Scala的OOP</h4><h2 id="Spark安装"><a href="#Spark安装" class="headerlink" title="Spark安装"></a>Spark安装</h2><h4 id="安装JAVA"><a href="#安装JAVA" class="headerlink" title="安装JAVA"></a>安装JAVA</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME	C:\Program Files\Java\jdk1.8.0_172</span><br><span class="line">PATH		%JAVA_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><p><a href="https://archive.apache.org/dist/hadoop/common/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME	D:\Software\hadoop-2.6.0</span><br><span class="line">PATH		%HADOOP_HOME%\bin</span><br></pre></td></tr></table></figure>
<p>ps: 对于windows平台需要到 <a href="https://github.com/steveloughran/winutils，下载对应版本的winutils" target="_blank" rel="noopener">https://github.com/steveloughran/winutils，下载对应版本的winutils</a></p>
<h4 id="安装Scala"><a href="#安装Scala" class="headerlink" title="安装Scala"></a>安装Scala</h4><p><a href="https://www.scala-lang.org/download/2.11.8.html" target="_blank" rel="noopener">https://www.scala-lang.org/download/2.11.8.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SCALA_HOME	C:\Program Files\Scala</span><br><span class="line">PATH		%SCALA_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h4><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME	D:\Software\spark-2.1.0-bin-hadoop2.6</span><br><span class="line">PATH		%SPARK_HOME%\bin</span><br></pre></td></tr></table></figure>
<h4 id="运行Spark"><a href="#运行Spark" class="headerlink" title="运行Spark"></a>运行Spark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master local[2]</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:4040/jobs/" target="_blank" rel="noopener">http://localhost:4040/jobs/</a></p>
<p>这样Spark就成功运行了。</p>
<p>PS For Linux:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/profile</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_144</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH</span><br><span class="line"></span><br><span class="line">export SCALA_HOME=/home/vaderwang/software/scala-2.11.8</span><br><span class="line">export PATH=$SCALA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/home/vaderwang/software/hadoop-2.7.3</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/home/vaderwang/software/spark-2.1.0</span><br><span class="line">export PATH=$SPARK_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>Spark Stand Alone运行</p>
<p>修改spark的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>
<p>在spark-env.sh中添加如下内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=localhost</span><br><span class="line">SPARK_WORKER_CORES=4</span><br><span class="line">SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_INSTANCES=1</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p> <a href="http://localhost:8080/" target="_blank" rel="noopener">http://localhost:8080/</a></p>
<p>配置单机多节点运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-all.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_HOST=localhost</span><br><span class="line">SPARK_WORKER_CORES=4</span><br><span class="line">SPARK_WORKER_MEMORY=4g</span><br><span class="line">SPARK_WORKER_INSTANCES=2</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark://localhost:7077</span><br></pre></td></tr></table></figure>
<h3 id="Spark-Core-RDD"><a href="#Spark-Core-RDD" class="headerlink" title="Spark Core RDD"></a>Spark Core RDD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_map</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        result = rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>).collect()</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_filter</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        result = rdd.map(<span class="keyword">lambda</span> x: x * <span class="number">2</span>).filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>).collect()</span><br><span class="line">        print(result)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_flat_map</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        <span class="comment"># [['hello', 'spark'], ['hello', 'java'], ['hello', 'python'], ['hello', 'kafka'], ['hello', 'storm']]</span></span><br><span class="line">        print(rdd.map(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br><span class="line">        <span class="comment"># ['hello', 'spark', 'hello', 'java', 'hello', 'python', 'hello', 'kafka', 'hello', 'storm']</span></span><br><span class="line">        print(rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br><span class="line">        split__map = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        print(split__map.collect())</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_group_by_key</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># [&#123;'java': [1]&#125;, &#123;'python': [1]&#125;, &#123;'storm': [1]&#125;, &#123;'hello': [1, 1, 1, 1, 1]&#125;, &#123;'spark': [1]&#125;, &#123;'kafka': [1]&#125;]</span></span><br><span class="line">        print(map_rdd.groupByKey().map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: list(x[<span class="number">1</span>])&#125;).collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_reduce_by_key</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># [&#123;'java': [1]&#125;, &#123;'python': [1]&#125;, &#123;'storm': [1]&#125;, &#123;'hello': [1, 1, 1, 1, 1]&#125;, &#123;'spark': [1]&#125;, &#123;'kafka': [1]&#125;]</span></span><br><span class="line">        group_rdd = map_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">        result = group_rdd.map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="keyword">False</span>).map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>]))</span><br><span class="line">        print(result.collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">group_by_key_word_count</span><span class="params">()</span>:</span></span><br><span class="line">        data = [<span class="string">'hello spark'</span>, <span class="string">'hello java'</span>, <span class="string">'hello python'</span>, <span class="string">'hello kafka'</span>, <span class="string">'hello storm'</span>]</span><br><span class="line">        rdd = sc.parallelize(data)</span><br><span class="line">        map_rdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">        result = map_rdd.groupByKey().map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: sum(list(x[<span class="number">1</span>]))&#125;)</span><br><span class="line">        print(result.collect())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_union</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    my_reduce_by_key()</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_count</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir)\</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>))\</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    output = counts.collect()</span><br><span class="line">    print(output)</span><br><span class="line">    sc.stop()</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_file</span><span class="params">(file_dir, save_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir) \</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>)) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b).saveAsTextFile(save_dir)</span><br><span class="line">    <span class="comment"># output = counts.collect()</span></span><br><span class="line">    <span class="comment"># print(counts)</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topN</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    counts = sc.textFile(file_dir) \</span><br><span class="line">        .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">'   '</span>)) \</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a, b: a + b)\</span><br><span class="line">        .map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="keyword">False</span>).map(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).take(<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># output = counts.collect()</span></span><br><span class="line">    print(counts)</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">(file_dir)</span>:</span></span><br><span class="line">    conf = SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"spark0401"</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line">    data = sc.textFile(file_dir)</span><br><span class="line">    data = data.map(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)[<span class="number">1</span>])</span><br><span class="line">    total = data.map(<span class="keyword">lambda</span> age: int(age)).reduce(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    avg = total / data.count()</span><br><span class="line">    print(avg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># main()</span></span><br><span class="line">    <span class="comment"># word_count('file:///home/vaderwang/PycharmProjects/Spark/*.txt')</span></span><br><span class="line">    <span class="comment"># save_file('file:///home/vaderwang/PycharmProjects/Spark/*.txt', 'file:///home/vaderwang/PycharmProjects/Spark/wc')</span></span><br><span class="line">    <span class="comment"># topN('file:///home/vaderwang/PycharmProjects/Spark/hello1.txt')</span></span><br><span class="line">    average(<span class="string">'file:///home/vaderwang/PycharmProjects/Spark/hello1.txt'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.iceberg</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * DataFrame API基本操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"DataFrameApp"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"data/people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出dataframe对应的schema信息</span></span><br><span class="line">    peopleDF.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出数据集的前20条记录</span></span><br><span class="line">    peopleDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//查询某列所有的数据： select name from table</span></span><br><span class="line">    peopleDF.select(<span class="string">"name"</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询某几列所有的数据，并对列进行计算： select name, age+10 as age2 from table</span></span><br><span class="line">    peopleDF.select(peopleDF.col(<span class="string">"name"</span>), (peopleDF.col(<span class="string">"age"</span>) + <span class="number">10</span>).as(<span class="string">"age2"</span>)).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据某一列的值进行过滤： select * from table where age&gt;19</span></span><br><span class="line">    peopleDF.filter(peopleDF.col(<span class="string">"age"</span>) &gt; <span class="number">19</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据某一列进行分组，然后再进行聚合操作： select age,count(1) from table group by age</span></span><br><span class="line">    peopleDF.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.iceberg</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * DataFrame中的操作操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameCase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"DataFrameRDDApp"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD ==&gt; DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(<span class="string">"data/student.data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注意：需要导入隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> studentDF = rdd.map(_.split(<span class="string">"\\|"</span>)).map(line =&gt; <span class="type">Student</span>(line(<span class="number">0</span>).toInt, line(<span class="number">1</span>), line(<span class="number">2</span>), line(<span class="number">3</span>))).toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//show默认只显示前20条</span></span><br><span class="line">    studentDF.show</span><br><span class="line">    studentDF.show(<span class="number">30</span>)</span><br><span class="line">    studentDF.show(<span class="number">30</span>, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    studentDF.take(<span class="number">10</span>)</span><br><span class="line">    studentDF.first()</span><br><span class="line">    studentDF.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    studentDF.select(<span class="string">"email"</span>).show(<span class="number">30</span>,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    studentDF.filter(<span class="string">"name=''"</span>).show</span><br><span class="line">    studentDF.filter(<span class="string">"name='' OR name='NULL'"</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//name以M开头的人</span></span><br><span class="line">    studentDF.filter(<span class="string">"SUBSTR(name,0,1)='M'"</span>).show</span><br><span class="line"></span><br><span class="line">    studentDF.sort(studentDF(<span class="string">"name"</span>)).show</span><br><span class="line">    studentDF.sort(studentDF(<span class="string">"name"</span>).desc).show</span><br><span class="line"></span><br><span class="line">    studentDF.sort(<span class="string">"name"</span>,<span class="string">"id"</span>).show</span><br><span class="line">    studentDF.sort(studentDF(<span class="string">"name"</span>).asc, studentDF(<span class="string">"id"</span>).desc).show</span><br><span class="line"></span><br><span class="line">    studentDF.select(studentDF(<span class="string">"name"</span>).as(<span class="string">"student_name"</span>)).show</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> studentDF2 = rdd.map(_.split(<span class="string">"\\|"</span>)).map(line =&gt; <span class="type">Student</span>(line(<span class="number">0</span>).toInt, line(<span class="number">1</span>), line(<span class="number">2</span>), line(<span class="number">3</span>))).toDF()</span><br><span class="line"></span><br><span class="line">    studentDF.join(studentDF2, studentDF.col(<span class="string">"id"</span>) === studentDF2.col(<span class="string">"id"</span>)).show</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, phone: <span class="type">String</span>, email: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.iceberg</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * DataFrame和RDD的互操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameRDDApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"DataFrameRDDApp"</span>).master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//inferReflection(spark)</span></span><br><span class="line"></span><br><span class="line">    program(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">program</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// RDD ==&gt; DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(<span class="string">"data/infos.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> infoRDD = rdd.map(_.split(<span class="string">","</span>)).map(line =&gt; <span class="type">Row</span>(line(<span class="number">0</span>).toInt, line(<span class="number">1</span>), line(<span class="number">2</span>).toInt))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> structType = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> infoDF = spark.createDataFrame(infoRDD,structType)</span><br><span class="line">    infoDF.printSchema()</span><br><span class="line">    infoDF.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过df的api进行操作</span></span><br><span class="line">    infoDF.filter(infoDF.col(<span class="string">"age"</span>) &gt; <span class="number">30</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过sql的方式进行操作</span></span><br><span class="line">    infoDF.createOrReplaceTempView(<span class="string">"infos"</span>)</span><br><span class="line">    spark.sql(<span class="string">"select * from infos where age &gt; 30"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inferReflection</span></span>(spark: <span class="type">SparkSession</span>) &#123;</span><br><span class="line">    <span class="comment">// RDD ==&gt; DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(<span class="string">"file:///Users/rocky/data/infos.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注意：需要导入隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> infoDF = rdd.map(_.split(<span class="string">","</span>)).map(line =&gt; <span class="type">Info</span>(line(<span class="number">0</span>).toInt, line(<span class="number">1</span>), line(<span class="number">2</span>).toInt)).toDF()</span><br><span class="line"></span><br><span class="line">    infoDF.show()</span><br><span class="line"></span><br><span class="line">    infoDF.filter(infoDF.col(<span class="string">"age"</span>) &gt; <span class="number">30</span>).show</span><br><span class="line"></span><br><span class="line">    infoDF.createOrReplaceTempView(<span class="string">"infos"</span>)</span><br><span class="line">    spark.sql(<span class="string">"select * from infos where age &gt; 30"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Info</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.iceberg</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Dataset操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DatasetApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"DatasetApp"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注意：需要导入隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> path = <span class="string">"data/sales.csv"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//spark如何解析csv文件？</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.option(<span class="string">"header"</span>,<span class="string">"true"</span>).option(<span class="string">"inferSchema"</span>,<span class="string">"true"</span>).csv(path)</span><br><span class="line">    df.show</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = df.as[<span class="type">Sales</span>]</span><br><span class="line">    ds.map(line =&gt; line.itemId).show</span><br><span class="line"></span><br><span class="line">    ds.map(line =&gt; line.itemId)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Sales</span>(<span class="params">transactionId:<span class="type">Int</span>,customerId:<span class="type">Int</span>,itemId:<span class="type">Int</span>,amountPaid:<span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="External-Data-Source-API"><a href="#External-Data-Source-API" class="headerlink" title="External Data Source API"></a>External Data Source API</h3><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>Spark MLlib</p>
<p>Spark GraphX</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2019-01-16</span><i class="fa fa-tag"></i></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2019/01/16/Spark/,啟,Spark,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2019/01/20/HIVE/" title="HIVE">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2019/01/07/HBase/" title="HBase">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>