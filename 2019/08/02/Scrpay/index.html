<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="王啟亮,2953381201@qq.com"><title>Scrapy · 啟</title><meta name="description" content="设置pip源设置pip源，临时指定pip源可以使用-i参数，设置默认需要在～文件目录下创建.pip文件夹，并在该文件夹中创建pip.conf文件，输入以下内容。
1mkdir ~/.pip
1touch pip.conf
123[global]trusted-host=mirrors.aliyun."><meta name="keywords" content="Java,Python,Javascript,Scala,Linux"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">啟</a></h3><div class="description"><p>Caring is not a luxury I can afford.</p></div></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/favicon.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Scrapy</a></h3></div><div class="post-content"><h3 id="设置pip源"><a href="#设置pip源" class="headerlink" title="设置pip源"></a>设置pip源</h3><p>设置pip源，临时指定pip源可以使用-i参数，设置默认需要在～文件目录下创建.pip文件夹，并在该文件夹中创建pip.conf文件，输入以下内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.pip</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch pip.conf</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br><span class="line">index-url=http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure>
<h3 id="安装Scrapy"><a href="#安装Scrapy" class="headerlink" title="安装Scrapy"></a>安装Scrapy</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.douban.com/simple scrapy</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
<h3 id="运行Scrapy"><a href="#运行Scrapy" class="headerlink" title="运行Scrapy"></a>运行Scrapy</h3><p>构建和运行一个爬虫只需完成以下几步：</p>
<ul>
<li>使用<code>scrapy startproject</code>创建爬虫模板或自己编写爬虫脚本</li>
<li>爬虫类继承<code>scrapy.Spider</code>，重写<code>parse</code>方法</li>
<li><code>parse</code>方法中<code>yield</code>或<code>return</code>字典、<code>Request</code>、<code>Item</code></li>
<li>使用<code>scrapy crawl &lt;spider_name&gt;</code>或<code>scrapy runspider &lt;spider_file.py&gt;</code>运行</li>
</ul>
<p>经过简单的几行代码，就能采集到某个网站下一些页面的数据，非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">"baidu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"www.baidu.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.baidu.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">      <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="scrapy调试"><a href="#scrapy调试" class="headerlink" title="scrapy调试"></a>scrapy调试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell https://www.baidu.com</span><br></pre></td></tr></table></figure>
<h3 id="Scrapy运行流程"><a href="#Scrapy运行流程" class="headerlink" title="Scrapy运行流程"></a>Scrapy运行流程</h3><p> <img src="/2019/08/02/Scrpay/./Scrpay/scrapy_architecture.png" alt="scrapy_architecture_02"></p>
<h1 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h1><p>Scrapy有以下几大组件：</p>
<ul>
<li><code>Scrapy Engine</code>：核心引擎，负责控制和调度各个组件，保证数据流转；</li>
<li><code>Scheduler</code>：负责管理任务、过滤任务、输出任务的调度器，存储、去重任务都在此控制；</li>
<li><code>Downloader</code>：下载器，负责在网络上下载网页数据，输入待下载URL，输出下载结果；</li>
<li><code>Spiders</code>：用户自己编写的爬虫脚本，可自定义抓取意图；</li>
<li><code>Item Pipeline</code>：负责输出结构化数据，可自定义输出位置；</li>
</ul>
<p>除此之外，还有两大中间件组件：</p>
<ul>
<li><code>Downloader middlewares</code>：介于引擎和下载器之间，可以在网页在下载前、后进行逻辑处理；</li>
<li><code>Spider middlewares</code>：介于引擎和爬虫之间，可以在调用爬虫输入下载结果和输出请求/数据时进行逻辑处理；</li>
</ul>
<h1 id="数据流转"><a href="#数据流转" class="headerlink" title="数据流转"></a>数据流转</h1><p>架构图中的数据流转是这样的：</p>
<ol>
<li><strong>Engine</strong>获取<strong>start_urls</strong>；</li>
<li><strong>Engine</strong>把该请求放入<strong>Scheduler</strong>中，同时<strong>Engine</strong>向<strong>Scheduler</strong>获取一个待下载的请求；</li>
<li><strong>Scheduler</strong>返回给<strong>Engine</strong>一个待下载的请求；</li>
<li><strong>Engine</strong>发送请求给<strong>Downloader</strong>，中间会经过一系列<strong>Downloader middlewares</strong>；</li>
<li>这个请求通过<strong>Downloader</strong>下载完成后，生成一个<strong>Response</strong>，返回给引擎，这中间会再次经过一系列<strong>Downloader middlewares</strong>；</li>
<li><strong>Engine</strong>接收到下载返回的响应对象后，然后发送给<strong>Spider</strong>，执行爬虫逻辑，中间会经过一系列<strong>Spider middlewares</strong>；</li>
<li><strong>Spider</strong>执行对应的回调方法，处理这个响应，完成爬虫逻辑后，会yield<strong>Item</strong>或<strong>Request</strong>给<strong>Engine</strong>，再次经过一系列<strong>Spider middlewares</strong>；</li>
<li><strong>Engine</strong>把返回的<strong>Item</strong>交由<strong>Item Pipeline</strong>处理，把<strong>Request</strong>通过引擎再交给<strong>Scheduler</strong>；</li>
</ol>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2019-08-02</span><i class="fa fa-tag"></i></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2019/08/02/Scrpay/,啟,Scrapy,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2019/08/05/Scrapy-Redis/" title="Scrapy-Redis">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2019/07/23/Spring-AOP/" title="Spring AOP">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>